{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU() if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelGate(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(ChannelGate, self).__init__()\n",
    "        self.gate_channels = gate_channels\n",
    "        self.mlp = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(gate_channels, gate_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_channels // reduction_ratio, gate_channels)\n",
    "            )\n",
    "        self.pool_types = pool_types\n",
    "    def forward(self, x):\n",
    "        channel_att_sum = None\n",
    "        for pool_type in self.pool_types:\n",
    "            if pool_type=='avg':\n",
    "                avg_pool = F.avg_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( avg_pool )\n",
    "            elif pool_type=='max':\n",
    "                max_pool = F.max_pool2d( x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n",
    "                channel_att_raw = self.mlp( max_pool )\n",
    "\n",
    "            if channel_att_sum is None:\n",
    "                channel_att_sum = channel_att_raw\n",
    "            else:\n",
    "                channel_att_sum = channel_att_sum + channel_att_raw\n",
    "                \n",
    "        scale = torch.sigmoid( channel_att_sum ).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
    "\n",
    "        return scale # Channel attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.cat( (torch.max(x,1)[0].unsqueeze(1), torch.mean(x,1).unsqueeze(1)), dim=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGate(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialGate, self).__init__()\n",
    "        kernel_size = 7\n",
    "        self.compress = ChannelPool()\n",
    "        conv_spatial = []\n",
    "        self.spatial = BasicConv(2,1,kernel_size,stride=1,padding=(kernel_size-1) // 2,relu=False, bn=False)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x1: Input vector\n",
    "        \"\"\"\n",
    "        x_compress = self.compress(x) # input\n",
    "        x_out = self.spatial(x_compress)\n",
    "            \n",
    "        scale = torch.sigmoid(x_out) # broadcasting\n",
    "        return scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBAM(nn.Module):\n",
    "    def __init__(self, gate_channels, reduction_ratio=16, pool_types=['avg', 'max']):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.ChannelGate = ChannelGate(gate_channels, reduction_ratio, pool_types)\n",
    "        self.SpatialGate = SpatialGate()\n",
    "    def forward(self,x1,x2):\n",
    "        \"\"\"\n",
    "        x1: Query vector\n",
    "        x2: Value vector\n",
    "        \"\"\"\n",
    "        c_map = self.ChannelGate(x1) # Channel attention map\n",
    "        s_map = self.SpatialGate(x1) # Spatial attention map\n",
    "        x_out = x2*c_map\n",
    "        x_out = x_out*s_map\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAM_Module(nn.Module):\n",
    "    #Ref from SAGAN\n",
    "    def __init__(self, in_dim,norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"\n",
    "        Position Attention Module\n",
    "        Arguments\n",
    "        in_dim: input channel dimension\n",
    "        ratio: ratio for level channels, used for reducing number of channels and parameters\n",
    "        rates: dilation rates\n",
    "        \n",
    "        Returns:\n",
    "        output: Output with attention module with parameter gamma for tuning\n",
    "        \"\"\"\n",
    "        super(PAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        self.query_conv = nn.Conv2d(self.channel_in,self.channel_in//8,kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(self.channel_in, self.channel_in//8, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x1 : input feature maps( B X C X H X W) ([SLF,MLF])\n",
    "                x2 : multi feature maps(B X C X H X W) (MLF)\n",
    "            returns :\n",
    "                out : scaled attention map\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x1.size()\n",
    "        proj_query = self.query_conv(x1).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x1).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "\n",
    "        out = torch.bmm(x2.view(m_batchsize,-1,width*height), attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "        out = self.gamma * out + x2\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAM_Module(nn.Module):\n",
    "    \"\"\" Channel attention module\"\"\"\n",
    "    def __init__(self, in_dim, norm_layer=nn.BatchNorm2d):\n",
    "        super(CAM_Module, self).__init__()\n",
    "        self.channel_in = in_dim\n",
    "        \n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax  = nn.Softmax(dim=-1)\n",
    "    def forward(self,x1, x2):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ----------\n",
    "            inputs :\n",
    "                x1 : input feature maps( B X C X H X W) ([SLF,MLF])\n",
    "                x2 : multi feature maps(B X C X H X W) (MLF)\n",
    "            returns :\n",
    "                out : attention value + input feature\n",
    "                attention: B X C X C\n",
    "        \"\"\"\n",
    "        m_batchsize, C, height, width = x1.size()\n",
    "           \n",
    "        # Flatten \n",
    "        proj_query = x1.view(m_batchsize,C,-1)\n",
    "        proj_key = x1.view(m_batchsize,C,-1).permute(0,2,1)\n",
    "        \n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        energy_new = torch.max(energy, -1, keepdim=True)[0].expand_as(energy)-energy\n",
    "        attention = self.softmax(energy_new)\n",
    "        \n",
    "        proj_value = x2.view(m_batchsize, C, -1)\n",
    "        out = torch.bmm(attention, proj_value)\n",
    "        out = out.view(m_batchsize, C, height, width)\n",
    "\n",
    "#         out = self.gamma * out + x2\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefineConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Helper function for Multiple Convolutions for refining.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    inputs:\n",
    "        in_ch : input channels\n",
    "        out_ch : output channels\n",
    "        attn : Boolean value whether to use Softmax or PReLU\n",
    "    outputs:\n",
    "        returns the refined convolution tensor\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch,norm_layer=nn.BatchNorm2d):\n",
    "        super(RefineConv, self).__init__()\n",
    "        \n",
    "        self.refine = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=1),\n",
    "            norm_layer(out_ch), \n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1), \n",
    "            norm_layer(out_ch), \n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1), \n",
    "            norm_layer(out_ch), \n",
    "            nn.PReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.refine(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseASPP(nn.Module):\n",
    "    def __init__(self,in_channels, level_channels, rates,norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"\n",
    "        in_channels: input channels of input\n",
    "        level_channels: level channels\n",
    "        rates: dilation rates\n",
    "        \"\"\"\n",
    "        super(DenseASPP,self).__init__()\n",
    "        self.rates = rates\n",
    "        self.in_channels = in_channels\n",
    "        self.level_channels = level_channels\n",
    "        convs = []\n",
    "        bns = []\n",
    "        prelus = []\n",
    "        c0 = in_channels // 2\n",
    "        \n",
    "        # Downsize of input\n",
    "        self.down0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,c0,1),\n",
    "            norm_layer(c0),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        for i in range(0,len(self.rates)):\n",
    "            temp_in_channels = c0 + i*level_channels\n",
    "            convs.append(nn.Conv2d(temp_in_channels,level_channels,3,dilation=self.rates[i],padding=self.rates[i]))\n",
    "            bns.append(norm_layer(level_channels))\n",
    "            prelus.append(nn.PReLU())\n",
    "        \n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.bns = nn.ModuleList(bns)\n",
    "        self.prelus = nn.ModuleList(prelus)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Transform to lower dimension\n",
    "        c0 = self.down0(x)\n",
    "        final_output = x\n",
    "        c_i = c0\n",
    "        for i in range(len(self.rates)):\n",
    "            temp_out = self.convs[i](c_i)\n",
    "            temp_out = self.bns[i](temp_out)\n",
    "            temp_out = self.prelus[i](temp_out)\n",
    "            \n",
    "            c_i = torch.cat((c_i,temp_out),dim=1)\n",
    "            final_output = torch.cat((final_output,temp_out),dim=1)\n",
    "            \n",
    "        return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAM_Module(\n",
      "  (query_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (key_conv): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (softmax): Softmax(dim=-1)\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d7b6612a9554>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPAM_Module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-53fcc78c841e>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x1, x2)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mproj_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_batchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mproj_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm_batchsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0menergy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproj_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproj_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menergy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":   \n",
    "    x = torch.randn((1,64,256,256))\n",
    "    y = torch.randn((1,64,256,256))\n",
    "    test = out(x,y)\n",
    "    total_params = sum(p.numel() for p in out.parameters())\n",
    "    print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
