{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models.resnet import ResNet, Bottleneck, BasicBlock\n",
    "from torch.nn import functional as F\n",
    "from Attention import *\n",
    "# from ResNets import resnet34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-34 feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_ResNet34(ResNet):\n",
    "    def __init__(self):\n",
    "        super(custom_ResNet34,self).__init__(BasicBlock,[3,4,6,3])\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x0 = self.relu(x)\n",
    "        x = self.maxpool(x0)\n",
    "        \n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "    \n",
    "        return x1, x2, x3, x4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Den$e Attention Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm_32(nn.GroupNorm):\n",
    "    def __init__(self,in_channels,num_groups=32):\n",
    "        super(GroupNorm_32,self).__init__(num_groups,in_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN_Refine(nn.Module):\n",
    "    def __init__(self,_norm_layer=nn.BatchNorm2d):\n",
    "        super(FPN_Refine,self).__init__()\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # MLF\n",
    "        self.fuse1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), _norm_layer(128), nn.PReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), _norm_layer(128), nn.PReLU()\n",
    "        )     \n",
    "        \n",
    "        # Fused attention\n",
    "        self.fuse2 = nn.Sequential(\n",
    "            nn.Conv2d(512,512,kernel_size=1),\n",
    "            _norm_layer(512),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        #Dual Attention layers\n",
    "        self.combine_1_1 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_1= PAM_Module(128, norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_1= CAM_Module(128)\n",
    "        \n",
    "        self.combine_1_2 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )  \n",
    "        self.pam_attention_1_2 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_2 = CAM_Module(128)\n",
    "        \n",
    "        self.combine_1_3 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_3 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_3 = CAM_Module(128)\n",
    "\n",
    "        self.combine_1_4 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_4 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_4 = CAM_Module(128)\n",
    "        \n",
    "        # Refinement layers\n",
    "        self.refine4 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine3 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine2 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine1 = RefineConv(256,128,_norm_layer)\n",
    "        \n",
    "        # DenseASPP module\n",
    "        self.dense_aspp_layers = nn.Sequential(\n",
    "            DenseASPP(512,64,(3,6,12,18),_norm_layer),\n",
    "            nn.Conv2d(768,128,kernel_size=1),\n",
    "            _norm_layer(128),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # Prediction layers\n",
    "        self.predict4 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "\n",
    "        self.predict4_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        self.predict = nn.Conv2d(128,1,kernel_size=1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "    def forward(self, x, size):\n",
    "        # Bottom up network\n",
    "        layer1, layer2, layer3, layer4 = x\n",
    "        # Top down network (FPN)\n",
    "        down4 = self.down4(layer4)\n",
    "        down3 = torch.add(\n",
    "            F.interpolate(down4,size=layer3.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down3(layer3)\n",
    "        )\n",
    "        down2 = torch.add(\n",
    "            F.interpolate(down3,size=layer2.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down2(layer2)\n",
    "        )\n",
    "        down1 = torch.add(\n",
    "            F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down1(layer1)\n",
    "        )\n",
    "        \n",
    "        down4 = F.interpolate(down4,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down3 = F.interpolate(down3,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down2 = F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        \n",
    "        # Deep supervision of top down network\n",
    "        predict4 = self.predict4(down4)\n",
    "        predict3 = self.predict3(down3)\n",
    "        predict2 = self.predict2(down2)\n",
    "        predict1 = self.predict1(down1)\n",
    "\n",
    "        fuse1 = self.fuse1(torch.cat((down4, down3, down2, down1), 1))\n",
    "        \n",
    "        # Attention layers\n",
    "        combine_4 = self.combine_1_4(torch.cat((down4,fuse1),1))\n",
    "        attn_pam4 = self.pam_attention_1_4(combine_4,fuse1)\n",
    "        attn_cam4 = self.cam_attention_1_4(combine_4,fuse1)\n",
    "        refine4 = self.refine4(torch.cat((down4,(attn_pam4+attn_cam4)),dim=1))\n",
    "        \n",
    "        combine_3 = self.combine_1_3(torch.cat((down3,fuse1),1))\n",
    "        attn_pam3 = self.pam_attention_1_3(combine_3,fuse1)\n",
    "        attn_cam3 = self.cam_attention_1_3(combine_3,fuse1)\n",
    "        refine3 = self.refine3(torch.cat((down3,(attn_pam3+attn_cam3)),dim=1))\n",
    "        \n",
    "        combine_2 = self.combine_1_2(torch.cat((down2,fuse1),1))\n",
    "        attn_pam2 = self.pam_attention_1_2(combine_2,fuse1)\n",
    "        attn_cam2 = self.cam_attention_1_2(combine_2,fuse1)\n",
    "        refine2 = self.refine2(torch.cat((down2,(attn_pam2+attn_cam2)),dim=1))\n",
    "\n",
    "        combine_1 = self.combine_1_1(torch.cat((down1,fuse1),1))\n",
    "        attn_pam1 = self.pam_attention_1_1(combine_1,fuse1)\n",
    "        attn_cam1 = self.cam_attention_1_1(combine_1,fuse1)\n",
    "        refine1 = self.refine1(torch.cat((down1,(attn_pam1+attn_cam1)),dim=1))\n",
    "        \n",
    "        # Fuse refined attention layers for Dense ASPP layer\n",
    "        fuse2 = self.fuse2(torch.cat((refine1,refine2,refine3,refine4),1))\n",
    "        dense_aspp = self.dense_aspp_layers(fuse2)\n",
    "        \n",
    "        # Prediction layers\n",
    "        \n",
    "        # Main prediction\n",
    "        predict = self.predict(dense_aspp)\n",
    "        # Deep supervision layers\n",
    "        predict4_2 = self.predict4_2(refine4)\n",
    "        predict3_2 = self.predict3_2(refine3)\n",
    "        predict2_2 = self.predict2_2(refine2)\n",
    "        predict1_2 = self.predict1_2(refine1)\n",
    "        \n",
    "        predict1 = F.interpolate(predict1, size=size, mode='bilinear',align_corners=True)\n",
    "        predict2 = F.interpolate(predict2, size=size, mode='bilinear',align_corners=True)\n",
    "        predict3 = F.interpolate(predict3, size=size, mode='bilinear',align_corners=True)\n",
    "        predict4 = F.interpolate(predict4, size=size, mode='bilinear',align_corners=True)\n",
    "\n",
    "        predict1_2 = F.interpolate(predict1_2, size=size, mode='bilinear',align_corners=True)\n",
    "        predict2_2 = F.interpolate(predict2_2, size=size, mode='bilinear',align_corners=True)\n",
    "        predict3_2 = F.interpolate(predict3_2, size=size, mode='bilinear',align_corners=True)\n",
    "        predict4_2 = F.interpolate(predict4_2, size=size, mode='bilinear',align_corners=True)\n",
    "    \n",
    "        predict = F.interpolate(predict, size=size, mode='bilinear',align_corners=True)\n",
    "        return predict1, predict2, predict3, predict4, predict1_2, predict2_2, predict3_2, predict4_2, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANet(nn.Module):\n",
    "    def __init__(self,_norm_layer=nn.BatchNorm2d):\n",
    "        super(DANet, self).__init__()\n",
    "        \n",
    "        # Encoder group\n",
    "        self.encoder = custom_ResNet34()\n",
    "        self.encoder.load_state_dict(models.resnet34(pretrained=True).state_dict())\n",
    "        self.encoder.fc = nn.Identity()\n",
    "        \n",
    "        # Decoder group\n",
    "        self.decoder = FPN_Refine()\n",
    "    def forward(self, x):\n",
    "        # Bottom up network\n",
    "        layer1, layer2, layer3, layer4 = self.encoder(x)\n",
    "        layers = [layer1, layer2, layer3, layer4]\n",
    "        predict1, predict2, predict3, predict4, predict1_2, predict2_2, predict3_2, predict4_2, predict = self.decoder(layers, x.size()[2:])\n",
    "        \n",
    "        return predict1, predict2, predict3, predict4, predict1_2, predict2_2, predict3_2, predict4_2, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26182259\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "DANet(\n",
      "  (encoder): custom_ResNet34(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (decoder): FPN_Refine(\n",
      "    (down4): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down2): Sequential(\n",
      "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (down1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (fuse1): Sequential(\n",
      "      (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (8): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (fuse2): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (combine_1_1): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (pam_attention_1_1): PAM_Module(\n",
      "      (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (cam_attention_1_1): CAM_Module(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (combine_1_2): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (pam_attention_1_2): PAM_Module(\n",
      "      (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (cam_attention_1_2): CAM_Module(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (combine_1_3): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (pam_attention_1_3): PAM_Module(\n",
      "      (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (cam_attention_1_3): CAM_Module(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (combine_1_4): Sequential(\n",
      "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PReLU(num_parameters=1)\n",
      "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (pam_attention_1_4): PAM_Module(\n",
      "      (query_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (key_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (cam_attention_1_4): CAM_Module(\n",
      "      (softmax): Softmax(dim=-1)\n",
      "    )\n",
      "    (refine4): RefineConv(\n",
      "      (refine): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): PReLU(num_parameters=1)\n",
      "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (refine3): RefineConv(\n",
      "      (refine): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): PReLU(num_parameters=1)\n",
      "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (refine2): RefineConv(\n",
      "      (refine): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): PReLU(num_parameters=1)\n",
      "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (refine1): RefineConv(\n",
      "      (refine): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): PReLU(num_parameters=1)\n",
      "        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (8): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (dense_aspp_layers): Sequential(\n",
      "      (0): DenseASPP(\n",
      "        (down0): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "        )\n",
      "        (convs): ModuleList(\n",
      "          (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
      "          (1): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "          (2): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "          (3): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "        )\n",
      "        (bns): ModuleList(\n",
      "          (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (prelus): ModuleList(\n",
      "          (0): PReLU(num_parameters=1)\n",
      "          (1): PReLU(num_parameters=1)\n",
      "          (2): PReLU(num_parameters=1)\n",
      "          (3): PReLU(num_parameters=1)\n",
      "        )\n",
      "      )\n",
      "      (1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): PReLU(num_parameters=1)\n",
      "    )\n",
      "    (predict4): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict4_2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict3_2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict2_2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict1_2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (predict): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = DANet()\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(pytorch_total_params)\n",
    "    \n",
    "    # Test function\n",
    "    x = torch.randn(1,3,256,256)\n",
    "    outputs = model(x)\n",
    "    for output in outputs:\n",
    "        print(output.size())\n",
    "        \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANet_R(nn.Module):\n",
    "    def __init__(self,_norm_layer=nn.BatchNorm2d,_dilated=True):\n",
    "        super(DANet_R, self).__init__()\n",
    "        self.encoder = resnet34(norm_layer=_norm_layer,dilated=_dilated)\n",
    "\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        # MLF\n",
    "        self.fuse1 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), _norm_layer(128), nn.PReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1), _norm_layer(128), nn.PReLU()\n",
    "        )     \n",
    "        \n",
    "        #Dual Attention layers\n",
    "        self.combine_1_1 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_1= PAM_Module(128, norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_1= CAM_Module(128)\n",
    "        \n",
    "        self.combine_1_2 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )  \n",
    "        self.pam_attention_1_2 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_2 = CAM_Module(128)\n",
    "        \n",
    "        self.combine_1_3 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_3 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_3 = CAM_Module(128)\n",
    "\n",
    "        self.combine_1_4 = nn.Sequential(\n",
    "            nn.Conv2d(256,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "            nn.Conv2d(128,128,kernel_size=3,padding=1),_norm_layer(128),nn.PReLU(),\n",
    "        )\n",
    "        self.pam_attention_1_4 = PAM_Module(128,norm_layer=_norm_layer)\n",
    "        self.cam_attention_1_4 = CAM_Module(128)\n",
    "        \n",
    "        # Refinement layers\n",
    "        self.refine4 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine3 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine2 = RefineConv(256,128,_norm_layer)\n",
    "        self.refine1 = RefineConv(256,128,_norm_layer)\n",
    "        # Prediction layers\n",
    "        self.predict4 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "\n",
    "        self.predict4_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1_2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        self.predict = nn.Conv2d(512,1,kernel_size=1)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom up network\n",
    "        layer0, layer1, layer2, layer3, layer4 = self.encoder(x)\n",
    "        # Top down network (FPN)\n",
    "        down4 = self.down4(layer4)\n",
    "        down3 = torch.add(\n",
    "            F.interpolate(down4,size=layer3.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down3(layer3)\n",
    "        )\n",
    "        down2 = torch.add(\n",
    "            F.interpolate(down3,size=layer2.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down2(layer2)\n",
    "        )\n",
    "        down1 = torch.add(\n",
    "            F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down1(layer1)\n",
    "        )\n",
    "        \n",
    "        down4 = F.interpolate(down4,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down3 = F.interpolate(down3,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down2 = F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        \n",
    "        # Deep supervision of top down network\n",
    "        predict4 = self.predict4(down4)\n",
    "        predict3 = self.predict3(down3)\n",
    "        predict2 = self.predict2(down2)\n",
    "        predict1 = self.predict1(down1)\n",
    "\n",
    "        fuse1 = self.fuse1(torch.cat((down4, down3, down2, down1), 1))\n",
    "        \n",
    "        # Attention layers\n",
    "        combine_4 = self.combine_1_4(torch.cat((down4,fuse1),1))\n",
    "        attn_pam4 = self.pam_attention_1_4(combine_4,fuse1)\n",
    "        attn_cam4 = self.cam_attention_1_4(combine_4,fuse1)\n",
    "        refine4 = self.refine4(torch.cat((down4,(attn_pam4+attn_cam4)),dim=1))\n",
    "        \n",
    "        combine_3 = self.combine_1_3(torch.cat((down3,fuse1),1))\n",
    "        attn_pam3 = self.pam_attention_1_3(combine_3,fuse1)\n",
    "        attn_cam3 = self.cam_attention_1_3(combine_3,fuse1)\n",
    "        refine3 = self.refine3(torch.cat((down3,(attn_pam3+attn_cam3)),dim=1))\n",
    "        \n",
    "        combine_2 = self.combine_1_2(torch.cat((down2,fuse1),1))\n",
    "        attn_pam2 = self.pam_attention_1_2(combine_2,fuse1)\n",
    "        attn_cam2 = self.cam_attention_1_2(combine_2,fuse1)\n",
    "        refine2 = self.refine2(torch.cat((down2,(attn_pam2+attn_cam2)),dim=1))\n",
    "\n",
    "        combine_1 = self.combine_1_1(torch.cat((down1,fuse1),1))\n",
    "        attn_pam1 = self.pam_attention_1_1(combine_1,fuse1)\n",
    "        attn_cam1 = self.cam_attention_1_1(combine_1,fuse1)\n",
    "        refine1 = self.refine1(torch.cat((down1,(attn_pam1+attn_cam1)),dim=1))\n",
    "        \n",
    "        # Main prediction \n",
    "        predict = self.predict(torch.cat((refine1,refine2,refine3,refine4),1))\n",
    "        \n",
    "        # Deep supervision layers\n",
    "        predict4_2 = self.predict4_2(refine4)\n",
    "        predict3_2 = self.predict3_2(refine3)\n",
    "        predict2_2 = self.predict2_2(refine2)\n",
    "        predict1_2 = self.predict1_2(refine1)\n",
    "        \n",
    "        predict1 = F.interpolate(predict1, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict2 = F.interpolate(predict2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict3 = F.interpolate(predict3, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict4 = F.interpolate(predict4, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "\n",
    "        predict1_2 = F.interpolate(predict1_2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict2_2 = F.interpolate(predict2_2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict3_2 = F.interpolate(predict3_2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict4_2 = F.interpolate(predict4_2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "    \n",
    "        predict = F.interpolate(predict, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        return predict1, predict2, predict3, predict4, predict1_2, predict2_2, predict3_2, predict4_2, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANet_Base(nn.Module):\n",
    "    def __init__(self,_norm_layer=nn.BatchNorm2d,_dilated=True):\n",
    "        super(DANet_Base, self).__init__()\n",
    "        self.encoder = resnet34(norm_layer=_norm_layer,dilated=_dilated)\n",
    "        \n",
    "        # down layers\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        # Prediction layers\n",
    "        self.predict4 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom up network\n",
    "        layer0, layer1, layer2, layer3, layer4 = self.encoder(x)\n",
    "        # Top down network (FPN)\n",
    "        down4 = self.down4(layer4)\n",
    "        down3 = torch.add(\n",
    "            F.interpolate(down4,size=layer3.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down3(layer3)\n",
    "        )\n",
    "        down2 = torch.add(\n",
    "            F.interpolate(down3,size=layer2.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down2(layer2)\n",
    "        )\n",
    "        down1 = torch.add(\n",
    "            F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down1(layer1)\n",
    "        )\n",
    "        \n",
    "        down4 = F.interpolate(down4,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down3 = F.interpolate(down3,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down2 = F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        \n",
    "        # Deep supervision of top down network\n",
    "        predict4 = self.predict4(down4)\n",
    "        predict3 = self.predict3(down3)\n",
    "        predict2 = self.predict2(down2)\n",
    "        predict1 = self.predict1(down1)\n",
    "        \n",
    "        predict1 = F.interpolate(predict1, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict2 = F.interpolate(predict2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict3 = F.interpolate(predict3, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict4 = F.interpolate(predict4, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "\n",
    "        return predict1, predict2, predict3, predict4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DANet_DASPP(nn.Module):\n",
    "    def __init__(self,_norm_layer=nn.BatchNorm2d,_dilated=False):\n",
    "        super(DANet_DASPP, self).__init__()\n",
    "        self.encoder = resnet34(norm_layer=_norm_layer,dilated=_dilated)\n",
    "\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1), _norm_layer(128), nn.PReLU()\n",
    "        )     \n",
    "        \n",
    "        # Prediction layers\n",
    "        self.predict4 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        self.dense_aspp_layers = nn.Sequential(\n",
    "            DenseASPP(128,64,(3,6,12,18),_norm_layer),\n",
    "            nn.Conv2d(384,128,kernel_size=1),\n",
    "            _norm_layer(128),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "        \n",
    "        self.predict4 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict3 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict2 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        self.predict1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        self.predict = nn.Conv2d(128,1,kernel_size=1)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.GroupNorm):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bottom up network\n",
    "        layer0, layer1, layer2, layer3, layer4 = self.encoder(x)\n",
    "        # Top down network (FPN)\n",
    "        down4 = self.down4(layer4)\n",
    "        down3 = torch.add(\n",
    "            F.interpolate(down4,size=layer3.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down3(layer3)\n",
    "        )\n",
    "        down2 = torch.add(\n",
    "            F.interpolate(down3,size=layer2.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down2(layer2)\n",
    "        )\n",
    "        down1 = torch.add(\n",
    "            F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True),\n",
    "            self.down1(layer1)\n",
    "        )\n",
    "        \n",
    "        down4 = F.interpolate(down4,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down3 = F.interpolate(down3,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        down2 = F.interpolate(down2,size=layer1.size()[2:],mode=\"bilinear\",align_corners=True)\n",
    "        \n",
    "        # Deep supervision of top down network\n",
    "        predict4 = self.predict4(down4)\n",
    "        predict3 = self.predict3(down3)\n",
    "        predict2 = self.predict2(down2)\n",
    "        predict1 = self.predict1(down1)\n",
    "        \n",
    "        # Main prediction \n",
    "        dense_aspp = self.dense_aspp_layers(down1)\n",
    "        predict = self.predict(dense_aspp)\n",
    "\n",
    "        # Deep supervision layers        \n",
    "        predict1 = F.interpolate(predict1, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict2 = F.interpolate(predict2, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict3 = F.interpolate(predict3, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        predict4 = F.interpolate(predict4, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "    \n",
    "        predict = F.interpolate(predict, size=x.size()[2:], mode='bilinear',align_corners=True)\n",
    "        return predict1, predict2, predict3, predict4, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21837135\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "DANet_DASPP(\n",
      "  (encoder): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (3): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (4): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (5): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn1): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
      "        (bn2): GroupNorm_32(32, 512, eps=1e-05, affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): Sequential(\n",
      "    (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (down3): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (down2): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (down1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "    (2): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (predict4): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (predict3): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (predict2): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (predict1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dense_aspp_layers): Sequential(\n",
      "    (0): DenseASPP(\n",
      "      (down0): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "      )\n",
      "      (convs): ModuleList(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(3, 3), dilation=(3, 3))\n",
      "        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6))\n",
      "        (2): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12))\n",
      "        (3): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18))\n",
      "      )\n",
      "      (bns): ModuleList(\n",
      "        (0): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (1): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (2): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "        (3): GroupNorm_32(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (prelus): ModuleList(\n",
      "        (0): PReLU(num_parameters=1)\n",
      "        (1): PReLU(num_parameters=1)\n",
      "        (2): PReLU(num_parameters=1)\n",
      "        (3): PReLU(num_parameters=1)\n",
      "      )\n",
      "    )\n",
      "    (1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (2): GroupNorm_32(32, 128, eps=1e-05, affine=True)\n",
      "    (3): PReLU(num_parameters=1)\n",
      "  )\n",
      "  (predict): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = DANet_DASPP(GroupNorm_32)\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(pytorch_total_params)\n",
    "    \n",
    "    # Test function\n",
    "    x = torch.randn(1,3,256,256)\n",
    "    outputs = model(x)\n",
    "    for output in outputs:\n",
    "        print(output.size())\n",
    "    \n",
    "    \n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
